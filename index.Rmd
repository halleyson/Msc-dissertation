---
title: "msc dissertation"
author: "200256823"
date: '''r Sys.Date()'''
output:
  html_document: default
  pdf_document: default
line-citations: apalike
linkcolor: blue
frontsize: 12pt
---

\newpage

# *Notes*{-}
1. The GitHub repository for this project could access through [here](https://github.com/halleyson/msc-dissertation.git)

# Loading data

Because the orginal data was not encoded in UFT-8, so after download the original data, has to open it with excel and re-save it with UFT-8 encoded .csv file. But if knit/publish is not required, then this process is not required, but for loading the data (using readr), it has to be read_csv, not read.csv. 

```{r, message=FALSE}

library(readr)

pennycook2 <- read_csv("Pennycooketal Study 2.csv")

# Quick check
head(pennycook2)
```

# General steps to regenerate headline result (study 2)
## data cleaning

```{r, message=FALSE, results='hide'}

# rename columns that contains special characters
colnames(pennycook2)
names(pennycook2)[1] <- "condition"


#remove a participants that did not answer or fail to record any questions

pennycook2 <- pennycook2[!is.na(pennycook2$Fake),]


#data description: condition 1 = control; condition 2 = treatment 

#Choose with the column position 
library("dplyr")
pennystudy2r1 <- pennycook2 %>% 
  dplyr::select(c(1, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 2, 3, 4, 57, 60, 250, 252, 284,))

#re-code sharing intentions (fake/real) using the scale midpoint (1-3 = 0, 4-6 = 1)
##(06/08/21 update: realize I could do it after change it from wide to long, then only the column: "measurement" needs to perform the change, for documentation reason, I leave it as it is.)

##Fake
pennystudy2r1$Fake[pennystudy2r1$Fake > 0 & pennystudy2r1$Fake < 4] <- 0
pennystudy2r1$Fake[pennystudy2r1$Fake > 3 & pennystudy2r1$Fake < 7] <- 1
pennystudy2r1$Fake

pennystudy2r1$Fake1_1[pennystudy2r1$Fake1_1 > 0 & pennystudy2r1$Fake1_1 < 4] <- 0
pennystudy2r1$Fake1_1[pennystudy2r1$Fake1_1 > 3 & pennystudy2r1$Fake1_1 < 7] <- 1

pennystudy2r1$Fake1_2[pennystudy2r1$Fake1_2 > 0 & pennystudy2r1$Fake1_2 < 4] <- 0
pennystudy2r1$Fake1_2[pennystudy2r1$Fake1_2 > 3 & pennystudy2r1$Fake1_2 < 7] <- 1

pennystudy2r1$Fake1_3[pennystudy2r1$Fake1_3 > 0 & pennystudy2r1$Fake1_3 < 4] <- 0
pennystudy2r1$Fake1_3[pennystudy2r1$Fake1_3 > 3 & pennystudy2r1$Fake1_3 < 7] <- 1

pennystudy2r1$Fake1_4[pennystudy2r1$Fake1_4 > 0 & pennystudy2r1$Fake1_4 < 4] <- 0
pennystudy2r1$Fake1_4[pennystudy2r1$Fake1_4 > 3 & pennystudy2r1$Fake1_4 < 7] <- 1

pennystudy2r1$Fake1_5[pennystudy2r1$Fake1_5 > 0 & pennystudy2r1$Fake1_5 < 4] <- 0
pennystudy2r1$Fake1_5[pennystudy2r1$Fake1_5 > 3 & pennystudy2r1$Fake1_5 < 7] <- 1

pennystudy2r1$Fake1_6[pennystudy2r1$Fake1_6 > 0 & pennystudy2r1$Fake1_6 < 4] <- 0
pennystudy2r1$Fake1_6[pennystudy2r1$Fake1_6 > 3 & pennystudy2r1$Fake1_6 < 7] <- 1

pennystudy2r1$Fake1_7[pennystudy2r1$Fake1_7 > 0 & pennystudy2r1$Fake1_7 < 4] <- 0
pennystudy2r1$Fake1_7[pennystudy2r1$Fake1_7 > 3 & pennystudy2r1$Fake1_7 < 7] <- 1

pennystudy2r1$Fake1_8[pennystudy2r1$Fake1_8 > 0 & pennystudy2r1$Fake1_8 < 4] <- 0
pennystudy2r1$Fake1_8[pennystudy2r1$Fake1_8 > 3 & pennystudy2r1$Fake1_8 < 7] <- 1

pennystudy2r1$Fake1_9[pennystudy2r1$Fake1_9 > 0 & pennystudy2r1$Fake1_9 < 4] <- 0
pennystudy2r1$Fake1_9[pennystudy2r1$Fake1_9 > 3 & pennystudy2r1$Fake1_9 < 7] <- 1

pennystudy2r1$Fake1_10[pennystudy2r1$Fake1_10 > 0 & pennystudy2r1$Fake1_10 < 4] <- 0
pennystudy2r1$Fake1_10[pennystudy2r1$Fake1_10 > 3 & pennystudy2r1$Fake1_10 < 7] <- 1

pennystudy2r1$Fake1_11[pennystudy2r1$Fake1_11 > 0 & pennystudy2r1$Fake1_11 < 4] <- 0
pennystudy2r1$Fake1_11[pennystudy2r1$Fake1_11 > 3 & pennystudy2r1$Fake1_11 < 7] <- 1

pennystudy2r1$Fake1_12[pennystudy2r1$Fake1_12 > 0 & pennystudy2r1$Fake1_12 < 4] <- 0
pennystudy2r1$Fake1_12[pennystudy2r1$Fake1_12 > 3 & pennystudy2r1$Fake1_12 < 7] <- 1

pennystudy2r1$Fake1_13[pennystudy2r1$Fake1_13 > 0 & pennystudy2r1$Fake1_13 < 4] <- 0
pennystudy2r1$Fake1_13[pennystudy2r1$Fake1_13 > 3 & pennystudy2r1$Fake1_13 < 7] <- 1

pennystudy2r1$Fake1_14[pennystudy2r1$Fake1_14 > 0 & pennystudy2r1$Fake1_14 < 4] <- 0
pennystudy2r1$Fake1_14[pennystudy2r1$Fake1_14 > 3 & pennystudy2r1$Fake1_14 < 7] <- 1

pennystudy2r1$Fake1_15[pennystudy2r1$Fake1_15 > 0 & pennystudy2r1$Fake1_15 < 4] <- 0
pennystudy2r1$Fake1_15[pennystudy2r1$Fake1_15 > 3 & pennystudy2r1$Fake1_15 < 7] <- 1


##Real
pennystudy2r1$Real[pennystudy2r1$Real > 0 & pennystudy2r1$Real < 4] <- 0
pennystudy2r1$Real[pennystudy2r1$Real > 3 & pennystudy2r1$Real < 7] <- 1
pennystudy2r1$Real

pennystudy2r1$Real1_1[pennystudy2r1$Real1_1 > 0 & pennystudy2r1$Real1_1 < 4] <- 0
pennystudy2r1$Real1_1[pennystudy2r1$Real1_1 > 3 & pennystudy2r1$Real1_1 < 7] <- 1

pennystudy2r1$Real1_2[pennystudy2r1$Real1_2 > 0 & pennystudy2r1$Real1_2 < 4] <- 0
pennystudy2r1$Real1_2[pennystudy2r1$Real1_2 > 3 & pennystudy2r1$Real1_2 < 7] <- 1

pennystudy2r1$Real1_3[pennystudy2r1$Real1_3 > 0 & pennystudy2r1$Real1_3 < 4] <- 0
pennystudy2r1$Real1_3[pennystudy2r1$Real1_3 > 3 & pennystudy2r1$Real1_3 < 7] <- 1

pennystudy2r1$Real1_4[pennystudy2r1$Real1_4 > 0 & pennystudy2r1$Real1_4 < 4] <- 0
pennystudy2r1$Real1_4[pennystudy2r1$Real1_4 > 3 & pennystudy2r1$Real1_4 < 7] <- 1

pennystudy2r1$Real1_5[pennystudy2r1$Real1_5 > 0 & pennystudy2r1$Real1_5 < 4] <- 0
pennystudy2r1$Real1_5[pennystudy2r1$Real1_5 > 3 & pennystudy2r1$Real1_5 < 7] <- 1

pennystudy2r1$Real1_6[pennystudy2r1$Real1_6 > 0 & pennystudy2r1$Real1_6 < 4] <- 0
pennystudy2r1$Real1_6[pennystudy2r1$Real1_6 > 3 & pennystudy2r1$Real1_6 < 7] <- 1

pennystudy2r1$Real1_7[pennystudy2r1$Real1_7 > 0 & pennystudy2r1$Real1_7 < 4] <- 0
pennystudy2r1$Real1_7[pennystudy2r1$Real1_7 > 3 & pennystudy2r1$Real1_7 < 7] <- 1

pennystudy2r1$Real1_8[pennystudy2r1$Real1_8 > 0 & pennystudy2r1$Real1_8 < 4] <- 0
pennystudy2r1$Real1_8[pennystudy2r1$Real1_8 > 3 & pennystudy2r1$Real1_8 < 7] <- 1

pennystudy2r1$Real1_9[pennystudy2r1$Real1_9 > 0 & pennystudy2r1$Real1_9 < 4] <- 0
pennystudy2r1$Real1_9[pennystudy2r1$Real1_9 > 3 & pennystudy2r1$Real1_9 < 7] <- 1

pennystudy2r1$Real1_10[pennystudy2r1$Real1_10 > 0 & pennystudy2r1$Real1_10 < 4] <- 0
pennystudy2r1$Real1_10[pennystudy2r1$Real1_10 > 3 & pennystudy2r1$Real1_10 < 7] <- 1

pennystudy2r1$Real1_11[pennystudy2r1$Real1_11 > 0 & pennystudy2r1$Real1_11 < 4] <- 0
pennystudy2r1$Real1_11[pennystudy2r1$Real1_11 > 3 & pennystudy2r1$Real1_11 < 7] <- 1

pennystudy2r1$Real1_12[pennystudy2r1$Real1_12 > 0 & pennystudy2r1$Real1_12 < 4] <- 0
pennystudy2r1$Real1_12[pennystudy2r1$Real1_12 > 3 & pennystudy2r1$Real1_12 < 7] <- 1

pennystudy2r1$Real1_13[pennystudy2r1$Real1_13 > 0 & pennystudy2r1$Real1_13 < 4] <- 0
pennystudy2r1$Real1_13[pennystudy2r1$Real1_13 > 3 & pennystudy2r1$Real1_13 < 7] <- 1

pennystudy2r1$Real1_14[pennystudy2r1$Real1_14 > 0 & pennystudy2r1$Real1_14 < 4] <- 0
pennystudy2r1$Real1_14[pennystudy2r1$Real1_14 > 3 & pennystudy2r1$Real1_14 < 7] <- 1

pennystudy2r1$Real1_15[pennystudy2r1$Real1_15 > 0 & pennystudy2r1$Real1_15 < 4] <- 0
pennystudy2r1$Real1_15[pennystudy2r1$Real1_15 > 3 & pennystudy2r1$Real1_15 < 7] <- 1


#create veracity column (Real - Fake)
pennystudy2r1['veracity']=pennystudy2r1['Real'] - pennystudy2r1['Fake']

#successful screen1 test (result = 2)
pennystudy2r1['screen1']=pennystudy2r1['screen1_4'] + pennystudy2r1['screen1_7']

#successful screen2 test (result = 2)
pennystudy2r1['screen2']=pennystudy2r1['screen2_3'] + pennystudy2r1['screen2_5']

#successful screen3 test (result = 3)
pennystudy2r1['screen3'] = na_if(pennystudy2r1$screen3_2,1)
pennystudy2r1['screen3'] = na_if(pennystudy2r1$screen3,2)
pennystudy2r1['screen3'] = na_if(pennystudy2r1$screen3,4)
pennystudy2r1['screen3'] = na_if(pennystudy2r1$screen3,5)
#check if screen 3 successful
mean(pennystudy2r1$screen3, na.rm = TRUE)

#re-code screen test: replace 2 and 3 to 1
pennystudy2r1["screen1"][pennystudy2r1["screen1"] == 2] <- 1
pennystudy2r1["screen2"][pennystudy2r1["screen2"] == 2] <- 1
pennystudy2r1["screen3"][pennystudy2r1["screen3"] == 3] <- 1

#replace NA to 0 in screen 1,2,3

pennystudy2r1[c("screen1","screen2","screen3")][is.na(pennystudy2r1[c("screen1","screen2","screen3")])] <- 0

#create dummy column, add screen 1,2,3 together. Screen = 3 indicate success in all attention check
pennystudy2r1['dummycheck']=pennystudy2r1['screen1'] + pennystudy2r1['screen2'] + pennystudy2r1['screen3']


#re-code condition control = 0 treatment = 1
pennystudy2r1["condition"][pennystudy2r1["condition"] == 1] <- 0
pennystudy2r1["condition"][pennystudy2r1["condition"] == 2] <- 1

#like the researcher did, change data frame from wide to long
##first drop unnecessary column
colnames(pennystudy2r1)

pennys2r1long <- subset(pennystudy2r1, select = c(1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,40,44))

##Add ID column
pennys2r1long <- tibble::rowid_to_column(pennys2r1long, "ID")

## change from wide to long
library(tidyr)

keycol <- "newstype"
valuecol <- "measurement"
gathercols <- c("Fake1_1","Fake1_2","Fake1_3","Fake1_4","Fake1_5","Fake1_6","Fake1_7","Fake1_8","Fake1_9","Fake1_10","Fake1_11","Fake1_12","Fake1_13","Fake1_14","Fake1_15","Real1_1","Real1_2","Real1_3","Real1_4","Real1_5","Real1_6","Real1_7","Real1_8","Real1_9","Real1_10","Real1_11","Real1_12","Real1_13","Real1_14","Real1_15")

pennys2r1long <- gather_(pennys2r1long, keycol, valuecol, gathercols)

##no idea why it doesn't show condition column (control vs treatment) so I merge original condition to the new one

merge(pennystudy2r1, pennys2r1long, by.x='condition', by.y='ID')

##organize data by ID, then reorder the column
pennys2r1long <- pennys2r1long[order(pennys2r1long$ID),]

pennys2r1long <- pennys2r1long[, c(1, 2, 8, 9, 7, 4, 3, 6, 5)]

##rename real and fake to avereal and avefake, change dummycheck to attentioncheck to make it clear

names(pennys2r1long)[6] <- "Real_ave"
names(pennys2r1long)[7] <- "Fake_ave"
names(pennys2r1long)[5] <- "attentioncheck"

##create a headline veracity column, fake news items code as 0, real news items code as 1

pennys2r1long <- pennys2r1long %>%
  mutate(headlineveracity = case_when(
    startsWith(newstype, "R") ~ "1",
    startsWith(newstype, "F") ~ "0"
    ))

##realise that original researcher exclude the observation with answer contains na, thus the obs needs to minus 23 to match original obs 25,627

pennys2r1long_nona <- pennys2r1long[!is.na(pennys2r1long$measurement),]

##separating the data based on number of successful attention check

### attention check above and equal to 1
pennys2r1longa1 <- subset(pennys2r1long, attentioncheck %in% c("1","2","3"))

### attention check above and equal to 2
pennys2r1longa2 <- subset(pennys2r1long, attentioncheck %in% c("2","3"))

### attention check equal to 3
pennys2r1longa3 <- subset(pennys2r1long, attentioncheck %in% c("3"))


###find out the number of participants (obs) for each attention check level (use it to compare with No of participants in Table S5).

No.participantsa1 <- subset(pennystudy2r1, dummycheck %in% c("1","2","3"))
No.participantsa2 <- subset(pennystudy2r1, dummycheck %in% c("2","3"))
No.participantsa3 <- subset(pennystudy2r1, dummycheck %in% c("3"))

```



## relase I should not dichotomise the scale as it loses info and reduces the power.
```{r, message=FALSE, results='hide'}
#recreate a data frame that does not rescale the data to either 0,1 but from 0 to 1,so divided each value by rating = (rating - 1)/5

pennystudy2r1nodi <- pennycook2 %>% 
 dplyr::select (c(1, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 2, 3, 4, 57, 60, 250, 252, 284,))


#successful screen1 test (result = 2)
pennystudy2r1nodi['screen1']=pennystudy2r1nodi['screen1_4'] + pennystudy2r1nodi['screen1_7']

#successful screen2 test (result = 2)
pennystudy2r1nodi['screen2']=pennystudy2r1nodi['screen2_3'] + pennystudy2r1nodi['screen2_5']

#successful screen3 test (result = 3)
pennystudy2r1nodi['screen3'] = na_if(pennystudy2r1nodi$screen3_2,1)
pennystudy2r1nodi['screen3'] = na_if(pennystudy2r1nodi$screen3,2)
pennystudy2r1nodi['screen3'] = na_if(pennystudy2r1nodi$screen3,4)
pennystudy2r1nodi['screen3'] = na_if(pennystudy2r1nodi$screen3,5)
#check if screen 3 successful
mean(pennystudy2r1nodi$screen3, na.rm = TRUE)

#re-code screen test: replace 2 and 3 to 1
pennystudy2r1nodi["screen1"][pennystudy2r1nodi["screen1"] == 2] <- 1
pennystudy2r1nodi["screen2"][pennystudy2r1nodi["screen2"] == 2] <- 1
pennystudy2r1nodi["screen3"][pennystudy2r1nodi["screen3"] == 3] <- 1

#re-code condition control = 0 treatment = 1
pennystudy2r1nodi["condition"][pennystudy2r1nodi["condition"] == 1] <- 0
pennystudy2r1nodi["condition"][pennystudy2r1nodi["condition"] == 2] <- 1

#replace NA to 0 in screen 1,2,3

pennystudy2r1nodi[c("screen1","screen2","screen3")][is.na(pennystudy2r1nodi[c("screen1","screen2","screen3")])] <- 0

#create dummy column, add screen 1,2,3 together. Screen = 3 indicate success in all attention check
pennystudy2r1nodi['dummycheck']=pennystudy2r1nodi['screen1'] + pennystudy2r1nodi['screen2'] + pennystudy2r1nodi['screen3']


#like the researcher did, change data frame from wide to long
##first drop unnecessary column
colnames(pennystudy2r1nodi)

pennys2r1longnodi <- subset(pennystudy2r1nodi, select = c(1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,43))

##Add ID column
pennys2r1longnodi <- tibble::rowid_to_column(pennys2r1longnodi, "ID")

## change from wide to long
library(tidyr)

keycol <- "newstype"
valuecol <- "measurement"
gathercols <- c("Fake1_1","Fake1_2","Fake1_3","Fake1_4","Fake1_5","Fake1_6","Fake1_7","Fake1_8","Fake1_9","Fake1_10","Fake1_11","Fake1_12","Fake1_13","Fake1_14","Fake1_15","Real1_1","Real1_2","Real1_3","Real1_4","Real1_5","Real1_6","Real1_7","Real1_8","Real1_9","Real1_10","Real1_11","Real1_12","Real1_13","Real1_14","Real1_15")

pennys2r1longnodi <- gather_(pennys2r1longnodi, keycol, valuecol, gathercols)

## I merge original condition to the new one

merge(pennystudy2r1nodi, pennys2r1longnodi, by.x='condition', by.y='ID')

##organize data by ID, then reorder the column
pennys2r1longnodi <- pennys2r1longnodi[order(pennys2r1longnodi$ID),]

pennys2r1longnodi <- pennys2r1longnodi[, c(1, 2, 7, 8, 6, 5, 4, 3)]

##change dummycheck to attentioncheck to make it clear

names(pennys2r1longnodi)[5] <- "attentioncheck"

##create a headline veracity column, fake news items code as 0, real news items code as 1

pennys2r1longnodi <- pennys2r1longnodi %>%
  mutate(headlineveracity = case_when(
    startsWith(newstype, "R") ~ "1",
    startsWith(newstype, "F") ~ "0"
    ))

##recode the measurement column

pennys2r1longnodi <- transform(pennys2r1longnodi, rescale = ave(measurement, condition, FUN = function(x) paste0((x-1)/5)))

##realise that original researcher exclude the observation with answer contains na, thus the obs needs to minus 23 to match original obs 25,627

pennys2r1longnodi_nona <- pennys2r1longnodi[!is.na(pennys2r1longnodi$measurement),]

##create a new column that recode item number from fake and real to 1-30, with 1-15 indicate each fake items, 16-30 indicate each real items, need it for clustering robust standard errors.

pennys2r1longnodi_nona <- within(pennys2r1longnodi_nona,{
  item_id <- NA
  item_id[newstype == "Fake1_1"] <- 1
  item_id[newstype == "Fake1_2"] <- 2
  item_id[newstype == "Fake1_3"] <- 3
  item_id[newstype == "Fake1_4"] <- 4
  item_id[newstype == "Fake1_5"] <- 5
  item_id[newstype == "Fake1_6"] <- 6
  item_id[newstype == "Fake1_7"] <- 7
  item_id[newstype == "Fake1_8"] <- 8
  item_id[newstype == "Fake1_9"] <- 9
  item_id[newstype == "Fake1_10"] <- 10
  item_id[newstype == "Fake1_11"] <- 11
  item_id[newstype == "Fake1_12"] <- 12
  item_id[newstype == "Fake1_13"] <- 13
  item_id[newstype == "Fake1_14"] <- 14
  item_id[newstype == "Fake1_15"] <- 15
  item_id[newstype == "Real1_1"] <- 16
  item_id[newstype == "Real1_2"] <- 17
  item_id[newstype == "Real1_3"] <- 18
  item_id[newstype == "Real1_4"] <- 19
  item_id[newstype == "Real1_5"] <- 20
  item_id[newstype == "Real1_6"] <- 21
  item_id[newstype == "Real1_7"] <- 22
  item_id[newstype == "Real1_8"] <- 23
  item_id[newstype == "Real1_9"] <- 24
  item_id[newstype == "Real1_10"] <- 25
  item_id[newstype == "Real1_11"] <- 26
  item_id[newstype == "Real1_12"] <- 27
  item_id[newstype == "Real1_13"] <- 28
  item_id[newstype == "Real1_14"] <- 29
  item_id[newstype == "Real1_15"] <- 30})




##subset based on attetnion check
attention1nodi <- subset(pennys2r1longnodi_nona, attentioncheck %in% c("1","2","3"))
attention2nodi <- subset(pennys2r1longnodi_nona, attentioncheck %in% c("2","3"))
attention3nodi <- subset(pennys2r1longnodi_nona, attentioncheck %in% c("3"))

```



## reproduction of results (interaction between headline veracity and treatment)
### without considering attention check
```{r, message=FALSE}
library(lmtest)
library(stargazer) 
library(sandwich)


lm1 <- lm(rescale ~ headlineveracity + headlineveracity*condition, data = pennys2r1longnodi_nona)
summary(lm1)

stargazer(lm1, type='text')

coeftest(lm1, vcov = vcovCL, cluster = ~ ID + item_id)



#calculate the effect size cohen's d
##since they need to compare true vs false headline in different condition, thus, create a new data frame that only contain treatment and control with the attention check above and equal to 2, as stated in the research

library(effsize)

##change headlineveracity and rescale to numeric
pennys2r1longnodi_nona <- transform(pennys2r1longnodi_nona, 
                   headlineveracity = as.numeric(headlineveracity),
                   rescale = as.numeric(rescale))

#check the class
sapply(pennys2r1longnodi_nona,mode)

#subset the table based on condition
pennys2nodicohenc <- subset(pennys2r1longnodi_nona, condition %in% c("0"))
pennys2nodicohent <- subset(pennys2r1longnodi_nona, condition %in% c("1"))

#condition vs measurement effect size, using participants' average measurement.
cohen.d(pennys2nodicohent$Real,pennys2nodicohent$Fake, na.rm = TRUE)
cohen.d(pennys2nodicohenc$Real,pennys2nodicohenc$Fake, na.rm = TRUE)


#Robustness for different levels of attentiveness
##attention above 1
lm2 <- lm(rescale ~ headlineveracity + headlineveracity*condition, data = attention1nodi)
summary(lm2)

stargazer(lm2, type='text')

coeftest(lm2, vcov = vcovCL, cluster = ~ ID + item_id)


##attention above 2
lm3 <- lm(rescale ~ headlineveracity + headlineveracity*condition, data = attention2nodi)
summary(lm3)

stargazer(lm3, type='text')

coeftest(lm3, vcov = vcovCL, cluster = ~ ID + item_id)


##attention all 3
lm4 <- lm(rescale ~ headlineveracity + headlineveracity*condition, data = attention3nodi)
summary(lm4)

stargazer(lm4, type='text')

coeftest(lm4, vcov = vcovCL, cluster = ~ ID + item_id)


```


## reproduction of figure 2 (without considering attention check)

### graph: including all
```{r, message=FALSE}

library(ggplot2)
library(DescTools)

#create a table contain mean and CI for each headline veracity in each condition
z <- with(pennys2r1long_nona, 
          aggregate(measurement, list(condition, headlineveracity), MeanCI))
z <- xtabs(x ~ Group.1 + Group.2, z)

z = data.frame(z)

#check the results with equation I know for sure
dummy <- aggregate(measurement~condition+headlineveracity, data = pennys2r1long_nona, FUN = mean)

#reshape the graph
z1 <- subset(z, Var3 %in% c("mean"))
z2 <- subset(z, Var3 %in% c("lwr.ci"))
z3 <- subset(z, Var3 %in% c("upr.ci"))

zsummary <- cbind(z1,z2$Freq,z3$Freq)


zsummary <- zsummary %>% 
  dplyr::select (c(1, 2, 4, 5, 6))


names(zsummary)[1] <- "condition"
names(zsummary)[2] <- "headlineveracity"
names(zsummary)[3] <- "percent"
names(zsummary)[4] <- "lowci"
names(zsummary)[5] <- "upci"

#change the value to %
zsummary$percent <- zsummary$percent*100
zsummary$lowci <- zsummary$lowci*100
zsummary$upci <- zsummary$upci*100
str(zsummary)


#plot the graph with error bar indicate CI

ggplot(zsummary, aes(x = condition, y = percent, fill = forcats::fct_rev(headlineveracity))) + 
  geom_bar(stat = 'identity',position = 'dodge',width = 0.5) + 
  geom_text(
    aes(label = sprintf("%.2f",percent)), vjust = 2.5, position = position_dodge(0.5)) +
  xlab("Condition") + 
  ylab("Likelihood of Sharing (%)") +
  scale_x_discrete(labels= c("0"="Control","1"="Treatment"))+
  scale_fill_manual(values = c("orange2","forestgreen"), name = "Headline Veracity", labels = c("True", "False")) + labs(title = "All Subjects") + 
  geom_errorbar(aes(ymin = lowci, ymax = upci,width = 0.2),position = position_dodge(0.5), size = 0.6)

##tips: forcats::fct_rev() is to reverse the order, use scale_fill_manual(values = to change colour

ggsave("All Subjects bar chart.png")



```


##Considering the likert scale should be treated as categorical rather than continuse, thus the ordinal logistic regression should be used, rather than normal linear regression


```{r, message=FALSE}
#Ordinal logistic regression


library(ordinal)
library(ggeffects)
library(ggrepel)

## All subjects
ordinaldata <- pennys2r1longnodi_nona

str(ordinaldata)
ordinaldata$measurement <- as.factor(ordinaldata$measurement)


ordinal <- clm(measurement ~ condition + condition*headlineveracity, data = ordinaldata, link = "logit")

summary(ordinal)

exp(coef(ordinal))

ggpredict_ordinal <- ggpredict(ordinal, terms = c("condition","headlineveracity"))
ggpredict_ordinaldata = data.frame(ggpredict_ordinal)

ggpredict_ordinal
str(ggpredict_ordinaldata)


ggpredict_ordinaldata$x = factor(ggpredict_ordinaldata$x)
levels(ggpredict_ordinaldata$x) = c("False", "True")
colnames(ggpredict_ordinaldata)[c(1,5,6)] =c("Headlineveracity", "Rating", "group")
levels(ggpredict_ordinaldata$group) = c("Control", "Treatment")
ggpredict_ordinaldata

ggplot(ggpredict_ordinaldata, aes(x = Rating, y = predicted)) + 
  geom_point(aes(color = Headlineveracity), position =position_dodge(width = 0.5)) + geom_errorbar(aes(ymin = conf.low, ymax = conf.high, color = Headlineveracity), position = position_dodge(width = 0.5), width = 0.3) + 
  theme_minimal() + facet_wrap(~group) + xlab("Likelihood of Sharing") + ylab("Predicted Probability of Sharing") + labs(title = "All Subjects") 
  
  
  + geom_label_repel(aes(label=sprintf("%.3f", predicted),fill = Headlineveracity), box.padding = 0.5, min.segment.length = unit(0, 'lines'), size = 3) #this gives you the box label of each points

ggsave("All Subjects.png")



## attention check1

ordinaldata1 <- attention1nodi

str(ordinaldata1)
ordinaldata1$measurement <- as.factor(ordinaldata1$measurement)


ordinal1 <- clm(measurement ~ condition + condition*headlineveracity, data = ordinaldata1, link = "logit")

summary(ordinal1)

exp(coef(ordinal1))

ggpredict_ordinal1 <- ggpredict(ordinal1, terms = c("condition","headlineveracity"))
ggpredict_ordinaldata1 = data.frame(ggpredict_ordinal1)

ggpredict_ordinal1
str(ggpredict_ordinaldata1)


ggpredict_ordinaldata1$x = factor(ggpredict_ordinaldata1$x)
levels(ggpredict_ordinaldata1$x) = c("False", "True")
colnames(ggpredict_ordinaldata1)[c(1,5,6)] =c("Headlineveracity", "Rating", "group")
levels(ggpredict_ordinaldata1$group) = c("Control", "Treatment")
ggpredict_ordinaldata1

ggplot(ggpredict_ordinaldata1, aes(x = Rating, y = predicted)) + 
  geom_point(aes(color = Headlineveracity), position =position_dodge(width = 0.5)) + geom_errorbar(aes(ymin = conf.low, ymax = conf.high, color = Headlineveracity), position = position_dodge(width = 0.5), width = 0.3) + 
  theme_minimal() + facet_wrap(~group) + xlab("Likelihood of Sharing") + ylab("Predicted Probability of Sharing") + labs(title = "\u2265 1 Screener Correct")


 + geom_label_repel(aes(label=sprintf("%.3f", predicted),fill = Headlineveracity), box.padding = 0.5, min.segment.length = unit(0, 'lines'), size = 3)

ggsave("\u2265 1 Screener Correct.png")


##tips, \u2265 could display greater and equal to sign



## attention check2

ordinaldata2 <- attention2nodi

str(ordinaldata2)
ordinaldata2$measurement <- as.factor(ordinaldata2$measurement)


ordinal2 <- clm(measurement ~ condition + condition*headlineveracity, data = ordinaldata2, link = "logit")

summary(ordinal2)

exp(coef(ordinal2))

ggpredict_ordinal2 <- ggpredict(ordinal2, terms = c("condition","headlineveracity"))
ggpredict_ordinaldata2 = data.frame(ggpredict_ordinal2)

ggpredict_ordinal2
str(ggpredict_ordinaldata2)


ggpredict_ordinaldata2$x = factor(ggpredict_ordinaldata2$x)
levels(ggpredict_ordinaldata2$x) = c("False", "True")
colnames(ggpredict_ordinaldata2)[c(1,5,6)] =c("Headlineveracity", "Rating", "group")
levels(ggpredict_ordinaldata2$group) = c("Control", "Treatment")
ggpredict_ordinaldata2

ggplot(ggpredict_ordinaldata2, aes(x = Rating, y = predicted)) + 
  geom_point(aes(color = Headlineveracity), position =position_dodge(width = 0.5)) + geom_errorbar(aes(ymin = conf.low, ymax = conf.high, color = Headlineveracity), position = position_dodge(width = 0.5), width = 0.3) + 
  theme_minimal() + facet_wrap(~group) + xlab("Likelihood of Sharing") + ylab("Predicted Probability of Sharing")  + labs(title = "\u2265 2 Screener Correct")
  

 + geom_label_repel(aes(label=sprintf("%.3f", predicted),fill = Headlineveracity), box.padding = 0.5, min.segment.length = unit(0, 'lines'), size = 3)

ggsave("\u2265 2 Screener Correct.png")



## attention check 3

ordinaldata3 <- attention3nodi

str(ordinaldata3)
ordinaldata3$measurement <- as.factor(ordinaldata3$measurement)


ordinal3 <- clm(measurement ~ condition + condition*headlineveracity, data = ordinaldata3, link = "logit")

summary(ordinal3)

exp(coef(ordinal3))

ggpredict_ordinal3 <- ggpredict(ordinal3, terms = c("condition","headlineveracity"))
ggpredict_ordinaldata3 = data.frame(ggpredict_ordinal3)

ggpredict_ordinal3
str(ggpredict_ordinaldata3)


ggpredict_ordinaldata3$x = factor(ggpredict_ordinaldata3$x)
levels(ggpredict_ordinaldata3$x) = c("False", "True")
colnames(ggpredict_ordinaldata3)[c(1,5,6)] =c("Headlineveracity", "Rating", "group")
levels(ggpredict_ordinaldata3$group) = c("Control", "Treatment")
ggpredict_ordinaldata3

ggplot(ggpredict_ordinaldata3, aes(x = Rating, y = predicted)) + 
  geom_point(aes(color = Headlineveracity), position =position_dodge(width = 0.5)) + geom_errorbar(aes(ymin = conf.low, ymax = conf.high, color = Headlineveracity), position = position_dodge(width = 0.5), width = 0.3) + 
  theme_minimal() + facet_wrap(~group) + xlab("Likelihood of Sharing") + ylab("Predicted Probability of Sharing") + labs(title = "All 3 Screener Correct")
  

 + geom_label_repel(aes(label=sprintf("%.3f", predicted),fill = Headlineveracity), box.padding = 0.55, min.segment.length = unit(0, 'lines'), size = 2.5) 

ggsave("All 3 Screener Correct.png")


```












### code testing (irrelevant)

###GLM (bad idea, because it dicotomize the results leads to loss information and verlidity)

## use glm, stepwise approach to see how each factor explaining the sharing intention. The family for the model is binomial since the dependent variable is dichotomous.
glm0 <- glm(measurement ~ 1, data = pennys2r1long_nona, family = binomial(link = "logit"))

glm1 <- glm(measurement ~ condition, data = pennys2r1long_nona, family = binomial(link = "logit"))

glm2 <- glm(measurement ~ condition + headlineveracity, data = pennys2r1long_nona, family = binomial(link = "logit"))

glm3 <- glm(measurement ~ condition + condition*headlineveracity, data = pennys2r1long_nona, family = binomial(link = "logit"))

glm4 <- glm(measurement ~ condition + headlineveracity + attentioncheck, data = pennys2r1long_nona, family = binomial(link = "logit"))


glm5 <- glm(measurement ~ condition + condition*headlineveracity + attentioncheck, data = pennys2r1long_nona, family = binomial(link = "logit"))

anova(glm0,glm1,glm2,glm3,glm4,test='LR')

##looking at summary glm4, it is cleat that condition does not have a significant effect on measurement
summary(glm5)

###odds ratios and 95% CI

exp(cbind(OR = coef(glm3), confint(glm3)))



##subset the attention based on attention check
attention1 <- subset(pennys2r1long_nona, attentioncheck %in% c("1","2","3"))
attention2 <- subset(pennys2r1long_nona, attentioncheck %in% c("2","3"))
attention3 <- subset(pennys2r1long_nona, attentioncheck %in% c("3"))


## Different attention check group
glm41 <- glm(measurement ~ condition + headlineveracity + attentioncheck, data = attention1, family = binomial(link = "logit"))
glm411 <- glm(measurement ~ condition + condition*headlineveracity, data = attention1, family = binomial(link = "logit"))

summary(glm41)
summary(glm411)


glm42 <- glm(measurement ~ condition + headlineveracity + attentioncheck, data = attention2, family = binomial(link = "logit"))
glm421 <- glm(measurement ~ condition + condition*headlineveracity + attentioncheck, data = attention2, family = binomial(link = "logit"))

summary(glm42)
summary(glm421)


glm43 <- glm(measurement ~ condition + headlineveracity, data = attention3, family = binomial(link = "logit"))
glm431 <- glm(measurement ~ condition + condition*headlineveracity, data = attention3, family = binomial(link = "logit"))

summary(glm43)
summary(glm431)
```

#old graph



#Find out the sum for each type of news in different condition

p1 <- aggregate(measurement~condition+headlineveracity, data = pennys2r1long_nona, na.rm = TRUE, FUN = sum)



#since the obs is 25,627, thus for each news in each condition, it observed 25,627/4 = 6406.75
p1 <- transform(p1, percent = ave(measurement, condition, FUN = function(x) paste0(round(x/6406.75, 4)*100)))

#plot the graph

ggplot(p1, aes(x = factor(condition), y = as.numeric(percent), fill = headlineveracity)) + 
  geom_bar(stat = 'identity',position = 'dodge') + 
  geom_text(
    aes(label = as.numeric (percent)), vjust = 1.5, position = position_dodge(0.9)) +
  xlab("Condition") + 
  ylab("Likelihood of Sharing (%)") +
  scale_x_discrete(labels= c("0"="Control","1"="Treatment"))+
  scale_fill_discrete(name = "Headline Veracity", labels = c("False", "True")) + labs(title = "All Subjects")


```
#z scored
scale(pennystudy2r1$D)


#Additional
##maybe a mixed effect models?

###create a new column that recode item number from fake and real to 1-30, with 1-15 indicate each fake items, 16-30 indicate each real items

pennys2r1long <- within(pennys2r1long,{
  item_id <- NA
  item_id[newstype == "Fake1_1"] <- 1
  item_id[newstype == "Fake1_2"] <- 2
  item_id[newstype == "Fake1_3"] <- 3
  item_id[newstype == "Fake1_4"] <- 4
  item_id[newstype == "Fake1_5"] <- 5
  item_id[newstype == "Fake1_6"] <- 6
  item_id[newstype == "Fake1_7"] <- 7
  item_id[newstype == "Fake1_8"] <- 8
  item_id[newstype == "Fake1_9"] <- 9
  item_id[newstype == "Fake1_10"] <- 10
  item_id[newstype == "Fake1_11"] <- 11
  item_id[newstype == "Fake1_12"] <- 12
  item_id[newstype == "Fake1_13"] <- 13
  item_id[newstype == "Fake1_14"] <- 14
  item_id[newstype == "Fake1_15"] <- 15
  item_id[newstype == "Real1_1"] <- 16
  item_id[newstype == "Real1_2"] <- 17
  item_id[newstype == "Real1_3"] <- 18
  item_id[newstype == "Real1_4"] <- 19
  item_id[newstype == "Real1_5"] <- 20
  item_id[newstype == "Real1_6"] <- 21
  item_id[newstype == "Real1_7"] <- 22
  item_id[newstype == "Real1_8"] <- 23
  item_id[newstype == "Real1_9"] <- 24
  item_id[newstype == "Real1_10"] <- 25
  item_id[newstype == "Real1_11"] <- 26
  item_id[newstype == "Real1_12"] <- 27
  item_id[newstype == "Real1_13"] <- 28
  item_id[newstype == "Real1_14"] <- 29
  item_id[newstype == "Real1_15"] <- 30
})


library(lme4) 
library(lmerTest)

#model 1, item_id indicates repeated measurements for each item
mult1 <- lmer (measurement ~ condition + condition*headlineveracity + (1|item_id),
             data = pennys2r1longnodi_nona)
summary(mult1)


### graph 2 attention check above and equal to 1



#condition 2: above and equal to 1

pennystudy2plot2 <- pennys2r1longa1 %>%
  select (c(1, 2, 3, 4))

#separating the sheet

pennystudy2plot2c <- subset(pennystudy2plot2, condition %in% c("0"))
pennystudy2plot2t <- subset(pennystudy2plot2, condition %in% c("1"))


#Find out the sum for each type of news in different condition
p2 <- aggregate(measurement~condition+newstype, data = pennystudy2plot2, FUN = sum)

##double check for correctness
sum(pennystudy2plot2$measurement,na.rm = TRUE)

#since the obs is 1664, thus 1664/2 = 832 participants. So to get percentage of sharing intention each fake and real news need to divided by 832/2 = 416
ggplot(p2, aes(x = condition, y = measurement/428, fill = newstype)) + 
  geom_bar(stat = 'identity',position = 'dodge') + 
  geom_text(
    aes(label = round(measurement/428, digits = 3)), vjust = 1.5, position = position_dodge(0.9))+
    scale_y_continuous(labels = scales::percent)
```

### graph 3 attention check above and equal to 2

#condition 3: above and equal to 2
pennystudy2plot3 <- pennys2r1longa2 %>%
  select (c(1, 2, 3, 4))


#Find out the sum for each type of news in different condition

p3 <- aggregate(measurement~condition+newstype, data = pennystudy2plot3, na.rm = TRUE, FUN = sum)
p3 <- transform(p3, percent = ave(measurement, condition, FUN = function(x) paste0(round(x/428, 4)*100)))


##double check for correctness
sum(pennystudy2plot3$measurement,na.rm = TRUE)

#plot the graph under the proportion of overall observation (856)
ggplot(p3, aes(x = condition, y = as.numeric(percent), fill = newstype)) + 
  geom_bar(stat = 'identity',position = 'dodge') + 
  geom_text(
    aes(label = as.numeric (percent)), vjust = 1.5, position = position_dodge(0.9))
```
### graph 3 attention check above and equal to 2: mean

#condition 3: above and equal to 2
pennystudy2plot3 <- pennys2r1longa2 %>%
  select (c(1, 2, 3, 4))

pennystudy2plot3c <- subset(pennystudy2plot3, condition %in% c("0"))
pennystudy2plot3t <- subset(pennystudy2plot3, condition %in% c("1"))


#Find out the sum and percentage for each type of news in each condition

##control, percentage = 658/2 = 329
p31 <- aggregate(measurement~condition+newstype, data = pennystudy2plot3c, na.rm = TRUE, FUN = sum)
p31 <- transform(p31, percent = ave(measurement, condition, FUN = function(x) paste0(round(x/329, 4)*100)))

##treatment, percentage = 634/2 = 317
p32 <- aggregate(measurement~condition+newstype, data = pennystudy2plot3t, na.rm = TRUE, FUN = sum)
p32 <- transform(p32, percent = ave(measurement, condition, FUN = function(x) paste0(round(x/317, 4)*100)))

#merge two table together under same column name
p3 <- rbind(p31,p32)

##double check for correctness
sum(pennystudy2plot3$measurement,na.rm = TRUE)

#plot the graph
ggplot(p3, aes(x = condition, y = as.numeric(percent), fill = newstype)) + 
  geom_bar(stat = 'identity',position = 'dodge') + 
  geom_text(
    aes(label = as.numeric (percent)), vjust = 1.5, position = position_dodge(0.9))
```


### graph 4 attention check equal to 3

#condition 3: equal to 3
pennystudy2plot4 <- pennys2r1longa3 %>%
  select (c(1, 2, 3, 4))

#Find out the sum for each type of news in different condition
p4 <- aggregate(measurement~condition+newstype, data = pennystudy2plot4, FUN = sum)

##double check
sum(pennystudy2plot4$measurement,na.rm = TRUE)

#since the obs is 460 for attention = 3, thus 460/2 = 230 participants. So to get percentage of sharing intention each fake and real news need to divided by 230/2 = 115
ggplot(p4, aes(x = condition, y = measurement/115, fill = newstype)) + 
  geom_bar(stat = 'identity',position = 'dodge') + 
  geom_text(
    aes(label = round(measurement/115, digits = 3)), vjust = 1.5, position = position_dodge(0.9))+
    scale_y_continuous(labels = scales::percent)
```


























